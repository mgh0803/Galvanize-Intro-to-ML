{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bookpage.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLWQF9_Df8Sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://towardsdatascience.com/an-introduction-to-web-scraping-with-python-a2601e8619e5\n",
        "main_url = 'http://books.toscrape.com/index.html'\n",
        "\n",
        "import requests\n",
        "result = requests.get(main_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTvqE__wg4EI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(result.text, 'html.parser')\n",
        "\n",
        "print(soup.prettify()[:1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyYskPZyhEIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getAndParseURL(url):\n",
        "  result = requests.get(url)\n",
        "  soup = BeautifulSoup(result.text, 'html.parser')\n",
        "  return(soup)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miMVIB9JhMOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soup.find(\"article\", class_ = \"product_pod\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK6X_QFMhQDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soup.find(\"article\", class_ = \"product_pod\").div.a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zydftEshT57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soup.find(\"article\", class_ = \"product_pod\").div.a.get('href')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm7fYAvfhZaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main_page_products_urls = [x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")]\n",
        "\n",
        "print(str(len(main_page_products_urls)) + \" fetched products URLs\")\n",
        "print(\"One example:\")\n",
        "main_page_products_urls[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdhy0lw_ho45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getBooksURLs(url):\n",
        "  soup = getAndParseURL(url)\n",
        "  # remove the index.html part of the base url before returning the results\n",
        "  return([\"/\".join(url.split(\"/\")[:-1]) + \"/\" + x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDG5ES1yh9BE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "categories_urls = [main_url + x.get('href') for x in soup.find_all(\"a\", href=re.compile(\"catalogue/category/books\"))]\n",
        "categories_urls = categories_urls[1:] # we remove the first one becuase it corresponds to all the books\n",
        "\n",
        "print(str(len(categories_urls)) + \" fetched categories URLs\")\n",
        "print(\"Some examples:\")\n",
        "categories_urls[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYj7sXPciSPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# store all the results in a list\n",
        "pages_urls = [main_url]\n",
        "\n",
        "soup = getAndParseURL(pages_urls[0])\n",
        "\n",
        "# while we get two matches, this means that the webpage contains a 'previous' and a 'next' button\n",
        "# if there is only one button, this means that we are either on the first page or the last page\n",
        "# we stop when we get to the last page\n",
        "\n",
        "while len(soup.findAll(\"a\", href=re.compile(\"page\"))) == 2 or len(pages_urls) == 1:\n",
        "  \n",
        "  # get the new complete url by adding the fetched url to the base url and removing the .html part\n",
        "  new_url = \"/\".join(pages_urls[-1].split(\"/\")[:-1]) + \"/\" + soup.findAll(\"a\", href=re.compile(\"page\")) [-1].get(\"href\")\n",
        "  \n",
        "  # add the URL to the list\n",
        "  pages_urls.append(new_url)\n",
        "  \n",
        "  # parse the next page\n",
        "  soup = getAndParseURL(new_url)\n",
        "  \n",
        "print(str(len(pages_urls)) + \" fetched URLs\")\n",
        "print(\"Some examples:\")\n",
        "pages_urls[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVEnDG5FjDcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = requests.get(\"http://books.toscrape.com/catalogue/page-50.html\")\n",
        "print(\"status code for page 50: \" + str(result.status_code))\n",
        "\n",
        "result = requests.get(\"http://books.toscrape.com/catalogue/page-51.html\")\n",
        "print(\"status code for page 51: \" + str(result.status_code))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ISlR2omjRGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pages_urls = []\n",
        "\n",
        "new_page= \"http://books.toscrape.com/catalogue/page-1.html\"\n",
        "while requests.get(new_page).status_code == 200:\n",
        "  pages_urls.append(new_page)\n",
        "  new_page = pages_urls[-1].split(\"-\")[0] + \"-\" + str(int(pages_urls[-1].split(\"-\")[1].split(\".\")[0]) + 1) + \".html\"\n",
        "  \n",
        "print(str(len(pages_urls)) + \" fetched URLs\")\n",
        "print(\"Some examples:\")\n",
        "pages_urls[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5LNL2PBjtLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "booksURLs = []\n",
        "for page in pages_urls:\n",
        "  booksURLs.extend(getBooksURLs(page))\n",
        "  \n",
        "print(str(len(booksURLs)) + \" fetched URLs\")\n",
        "print(\"Some examples:\")\n",
        "booksURLs[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1teTa57ej5mR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = []\n",
        "prices = []\n",
        "nb_in_stock = []\n",
        "img_urls = []\n",
        "categories = []\n",
        "ratings = []\n",
        "\n",
        "# scrape data for every book in url: this may take some time\n",
        "for url in booksURLs:\n",
        "  soup = getAndParseURL(url)\n",
        "  # product name\n",
        "  names.append(soup.find(\"div\", class_ = re.compile(\"product_main\")).h1.text)\n",
        "  # product price\n",
        "  prices.append(soup.find(\"p\", class_=\"price_color\").text[2:]) # get rid of the pound sign\n",
        "  # number of available products\n",
        "  nb_in_stock.append(re.sub(\"[&0-9]\", \"\", soup.find(\"p\", class_ = \"instock availability\").text))\n",
        "  # image url\n",
        "  img_urls.append(url.replace(\"index.html\", \"\") + soup.find(\"img\").get(\"src\"))\n",
        "  # product category\n",
        "  categories.append(soup.find(\"a\", href = re.compile(\"../category/books\")).get(\"href\").split(\"/\")[3])\n",
        "  # ratings\n",
        "  ratings.append(soup.find(\"p\", class_ = re.compile(\"star-rating\")).get(\"class\")[1])\n",
        "  \n",
        " # add data into pandas df\n",
        "import pandas as pd\n",
        "\n",
        "scraped_data = pd.DataFrame({'name': names, 'price': prices, 'nb_in_stock': nb_in_stock, 'url_img':img_urls, 'product_category': categories, 'rating': ratings})\n",
        "scraped_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}